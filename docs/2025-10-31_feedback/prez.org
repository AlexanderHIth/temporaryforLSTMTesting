#+TITLE: Long horizon segmentation
#+author: Andrea PierrÃ©
#+date: November 3, 2025

* HEADER :noexport:
#+SETUPFILE: ./style.org

* Context
** Robot learning of long horizon demonstrations
#+begin_export latex
\begin{center}
\movie{\includegraphics[height=0.4\textheight]{medias/thumb.png}}{medias/rosbag2_2025-09-08_19-46-18_2025-09-08-19-46-19.mkv}
\end{center}
#+end_export


*** Left :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
[[file:medias/ground-truth-segmentation-higher-level.png]]
*** Right :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
[[file:medias/ground-truth-segmentation-lower-level.png]]


#+begin_export latex
\begin{textblock}{15}(1.2, 14.7)%
\rightarrow~Extremely \alert{small} data \ie{} few demonstrations
\end{textblock}
#+end_export

* Unsupervised learning -- State Detection
** H(S)MM -- Higher level movements {{{faicon(close)}}}
*** Left :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
[[file:medias/unsupervised-HMM.png]]
[[file:medias/HSMM.png]]
*** Right :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
[[file:medias/GP-HSMM.png]]
[[file:medias/ground-truth-segmentation-higher-level_old.png]]
* Unsupervised learning -- Change Point Detection
** Greedy Gaussian Segmentation -- Higher level movements {{{faicon(close)}}}
[[file:medias/GGS.png]]
#+ATTR_LaTeX: :height 0.45\textheight
[[file:medias/ground-truth-segmentation-higher-level_old.png]]
** ruptures -- Lower level movements
[[file:medias/ruptures.png]]
* Supervised learning
** Supervised HMM -- Lower level movements {{{faicon(check-circle-o)}}}
#+ATTR_LaTeX: :height 0.45\textheight
[[file:medias/ground-truth-segmentation-lower-level.png]]
#+ATTR_LaTeX: :height 0.45\textheight
[[file:medias/supervised-HMM-cont-lower-level.png]]
** Supervised HMM -- Higher level movements {{{faicon(close)}}}
#+ATTR_LaTeX: :height 0.45\textheight
[[file:medias/ground-truth-segmentation-higher-level.png]]
#+ATTR_LaTeX: :height 0.45\textheight
[[file:medias/supervised-HMM-cont-higher-level.png]]
** XGBoost -- Higher level movements {{{faicon(close)}}}
| Split | F1       | Accuracy |
|-------+----------+----------|
| $1$   | $0.2722$ | $0.3741$ |
| $2$   | $0.1639$ | $0.1872$ |
| $3$   | $0.1684$ | $0.3381$ |
| $4$   | $0.1469$ | $0.3501$ |
| $5$   | $0.1713$ | $0.3458$ |

* Conclusion
** Conclusion
- Can model *lower* level movements with good performance
- Modeling *higher* level movements remains a challenge
** Questions
- Any other method I may have missed that could help solve this problem?
  - Variational inference?
- How to integrate context, {{{EG}}} video?
  - Multimodal transformer but probably too data hungry?
** \nbsp{}
:PROPERTIES:
:BEAMER_opt: standout
:END:
Questions ?
* COMMENT Add plain option to Beamer TOC
% Local variables:
% org-beamer-outline-frame-options: "plain"
% End:

* COMMENT Feedback
** Ma
*** Easy solution \to force the higher level movement to be composed of 2 lower level movements
*** Small RNN
*** Hybrid HMM + RNN
*** KNN to find higher level movements
*** Map a new timeseries tuple to known timeseries classes with similarity measure
**** Basically Brendan's paper
**** Moving window to compare to previous segments
** Lab meeting
*** Pretrained model for object detection
*** Context by giving composition of primitive movements
*** Partially observable H(S)MM
*** Compare dataset to pretrain then finetune on own data
*** KILL RL?
Issue of how you define the reward signal

** Reza
*** DONE Train on 3 demonstrations, test on 1
**** DONE Need to make the ground truth
*** TODO Signal that tells if any object is moving or not
**** April tag on object to avoid needing a complex segmentation pipeline?
*** Change point detection signal to help the HMM?
** Thoughts
*** Parallel processes:
1. Output states from symbolic planner
2. Detect states with HMM
\to Mapping 1 to 1: Can compose new movements from symbolic planner
\to No Mapping == potential new skill?
*** Generate movements directly from learned HMM's generative process? (no DMP or ProMP) -> need to try, not sure it can work
